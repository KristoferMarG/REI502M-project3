---
title: "Project 3 - Cluster Analysis"
author: 
  - Jón Þorsteinsson - jth56@hi.is
  - Kristófer Már Gíslason - kmg14@hi.is
output: 
  html_document:
    css: styles.css
    theme: lumen
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
library(merTools)
library(dendextend)
library(colorspace) # get nice colors
```

### 1. Objectives

Our main obective was to determine with the highest possible accuracy what type of glass is found. Our ideal results would include 7 well defined clusters where each cluster would represent a glass type. The reason this can be found usefull is because of criminological investigations. Where at a scene of a crime glass can be correctly identified and possibly be used as valuable evidence.




### 2. Data set description

Without any preprocessing our data set has 214 different instances that each contain 10 different attributes and one class attribute that tells us what kind of glass it is from the 7 possible options.

| Nr. | Name | Type | Description |
| --- | ---- | ---- | ----------- |
| 1. | Id| numerical | number: 1 to 214 |
| 2. | RI| numerical | Refractive Index |
| 3. | Na| numerical | Sodium |
| 4. | Mg| numerical | Magnesium |
| 5. | Al| numerical | Aluminum |
| 6. | Si| numerical | Silicon |
| 7. | K | numerical | Potassium |
| 8. | Ca| numerical | Calcium|
| 9. | Ba |numerical | Barium |
| 10. |Fe |numerical | Iron| 
| 11. | Class | nominal | class - (building_windows_float_processed,building_windows_non_float_processed,vehicle_windows_float_processed, vehicle_windows_non_float_processed,containers,tableware,headlamps)|

#Preprocessing

We began by looking at all our attributes and noticed that the first one **Id** is completely useless to us because it only numbers the instances which we don't need. We also noticed that the class attributes never results to the glass being *vehicle_windows_non_float_processed* for this dataset so we should get one fewer clusters in the final result. All other attributes seem like they might have some interesting information so we include them. It is also imperative to normalize the scale of feature values in order to begin with the clustering process. This is because each observations' feature values are represented as coordinates in n-dimensional space



```{r data, tidy = TRUE}


# read in data file
glass <- read.csv("glass.csv")

# remove the Id attribute
glass <- glass[2:11]

# normalizing the attributes
glass[1,9] <- scale(glass[1,9])

```

### 3. K-means clustering

Now that we have finished preprocessing the next step is to run the k-means algorithm but first we need to find what number of clusters that will give us the best result we accomplish this by plotting the sum of squares and seeing when the drop decreases. Since the initial cluster assignments are random, we need to set the seed to ensure reproducibility. 
By plotting the sum of squares we can see that the drop of the y-axis decreases when the number of clusters is 6 this is ideal because the possible types of glass is also 6. 



```{r kmeans}


# Determine number of clusters
wss <- (nrow(glass)-2)*sum(apply(glass,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(glass,centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="sum of squares")

# Ensuring reproducibility
set.seed(20)
# running k-means 
glassClusterKmeans <- kmeans(glass, 6, nstart = 20, iter.max=100)


```

 
We used Principal component analysis (PCA) algorithm in **clusplot** to plot and view the clusters after the k-means algorithm. PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables.

```{r hierarchical}


#glass <- scale(glass) # standardize variables
# random subsample from dataset
#randsample <- glass[sample(1:nrow(glass), 20 , replace=FALSE),]


# plotting the clusters
clusplot(glass, glassClusterKmeans$cluster,
         main='Cluster solution',
         color=TRUE, shade=TRUE, labels=2, lines=0)


# reviewing the total sum of squared errors  
glass_kmeans_error <- sum(glassClusterKmeans$withinss)




```

53.47% of variability says that, with our data, more than half of the information about the multivariate data is captured by this plot of components 1 and 2.



### 4. Hierarchical Clustering

```{r results="hide",warning=FALSE}

d_iris <- dist(glass) # method="man" # is a bit better
hc_iris <- hclust(d_iris, method = "complete")
iris_species <- rev(levels(iris[,5]))


dend <- as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]

# We shall add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")
# We hang the dendrogram a bit:
dend <- hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
# dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
dend <- set(dend, "labels_cex", 0.5)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set
     (the labels give the true flower species)", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(6))



```



### 5. Comparison of K-means and Hierarchical

# Ward Hierarchical Clustering
d <- dist(glass, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D") 
plot(fit) # display dendogram
groups <- cutree(fit, k=7) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(fit, k=7, border="blue")








